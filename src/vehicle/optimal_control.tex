\chapter{Introduction à la commande optimale}

\begin{chapterintro}
    \introcontext{
        Jusqu'à présent, nous avons abordé la \textbf{génération de trajectoires} et la \textbf{planification de mouvement} pour résoudre l'aspect géométrique du problème : « comment aller d'un point A à un point B », mais sans vérifier si une séquence d'actions permette d’exécuter cette séquence. Du côté de la commande, plusieurs lois de commande ont été proposées qui donnent des garanties de convergence vers une position cible, mais typiquement sans tenir compte des contraintes, ni de l’efficacité de la solution.
        
        La commande optimale nous offre le cadre mathématique pour passer du simple mouvement admissible géométriquement au mouvement dynamique « idéal » (temps minimum, consommation d'énergie minimale, etc.). Ce chapitre introduit les fondements théoriques nécessaires pour traiter le système sous sa forme d'état générique $\dot{\col{x}} = \col{f}(\col{x}, \col{u}, t)$.
    }
    
    \introobjectives{
        À la fin de ce chapitre, vous serez capable de :
        \begin{itemize}
            \item Formuler mathématiquement un problème de commande optimale.
            \item Identifier les critères de performance (coûts) appropriés pour différentes tâches robotiques.
        \end{itemize}
    }
    
    \introprerequis{
        \begin{itemize}
            \item Espace d'état et représentations vectorielles.
        \end{itemize}
    }
\end{chapterintro}


\video{Introduction à la commande optimale}{https://youtu.be/3x6Vg-RRZ50}



\section{Formalisation mathématique}

La plupart des méthodes de commande optimale sont conçues pour travailler avec la forme la plus générale des équations différentielles, permettant de traiter une vaste gamme de systèmes robotiques.

\video{Exemple de loi de commande optimale pour un double intégrateur}{https://youtu.be/wKjEAXFvXlQ}


\begin{definition}{Le problème de commande optimale}{def:formalisation_opt}
    Le problème de commande optimale est défini par 
    une dynamique :
    \begin{equation}
        \dot{\col{x}} = f( \col{x}, \col{u}, t)
    \end{equation}
    des contraintes sur les états et les actions (limites d'actionnement, butées) :
    \begin{align}
        \col{x} \in \mathcal{X}_{ok} \\
        \col{u} \in \mathcal{U}_{ok}
    \end{align}
    un objectif caractérisé par une fonction de coût $J$ à minimiser :
    \begin{equation}
        J = \int_{t_0}^{t_f} g(\col{x},\col{u} )dt + h( \col{x}({t_f}) )
    \end{equation}
    et une (ou une distribution de) conditions initiales $\col{x}_0$.
\end{definition}

Dans cette formulation, $g(\col{x},\col{u})$ représente le coût instantané (ex : consommation d'énergie à chaque instant) et $h(\col{x}_f)$ représente le coût terminal (ex : précision de l'arrivée au point cible).



\section{Théorie de la commande optimale}

L'analyse théorique repose sur deux piliers : l'un traitant le problème de manière globale (HJB) et l'autre de manière locale (PMP).

\subsection{L'équation de Hamilton-Jacobi-Bellman (HJB)}

L'approche par programmation dynamique utilise le principe d'optimalité de Bellman pour définir la fonction de valeur optimale $J^*(\col{x}, t) = \min_{\col{u}} J$.

\begin{definition}{Équation de Hamilton-Jacobi-Bellman}{def:hjb_formal}
    La fonction de valeur $J^*$ doit satisfaire l'équation aux dérivées partielles :
    \begin{equation}
        -\frac{\partial J^*}{\partial t} = \min_{\col{u} \in \mathcal{U}_{ok}} \left[ g(\col{x}, \col{u}) + \left( \frac{\partial J^*}{\partial \col{x}} \right)^T f(\col{x}, \col{u}, t) \right]
    \end{equation}
    avec la condition terminale $J^*(\col{x}, t_f) = h(\col{x}(t_f))$.
\end{definition}

\subsection{Le Principe du Maximum de Pontryagin (PMP)}

Le PMP définit les conditions nécessaires pour qu'une trajectoire soit optimale en introduisant un vecteur de variables adjointes $\col{\lambda}$ (co-états).

\begin{definition}{Le Hamiltonien et le PMP}{def:pmp_formal}
    On définit le Hamiltonien $H$ comme :
    \begin{equation}
        H(\col{x}, \col{u}, \col{\lambda}, t) = g(\col{x}, \col{u}) + \col{\lambda}^T f(\col{x}, \col{u}, t)
    \end{equation}
    Une trajectoire optimale doit satisfaire :
    \begin{enumerate}
        \item \textbf{Équation du co-état :} $\dot{\col{\lambda}} = -\frac{\partial H}{\partial \col{x}}$
        \item \textbf{Équation d'état :} $\dot{\col{x}} = \frac{\partial H}{\partial \col{\lambda}}$
        \item \textbf{Stationnarité :} $\col{u}^* = \arg \min_{\col{u} \in \mathcal{U}_{ok}} H(\col{x}, \col{u}, \col{\lambda}, t)$
    \end{enumerate}
\end{definition}

\section{Grandes familles de méthodes}

Pour résoudre un problème de commande optimale, le choix fondamental réside dans la définition de l'\textbf{espace de décision}. On distingue deux philosophies majeures selon que l'on cherche une solution globale (une loi de commande) ou une solution locale (une trajectoire spécifique).

\subsection{Approches basées sur la loi de commande}

Ces méthodes visent à trouver une loi de commande optimale $\pi^*$, capable de définir l'action optimale pour n'importe quel état $\col{x}$ rencontré.

\begin{definition}{Programmation dynamique et apprentissage par renforcement}{def:dp_rl_formal}
    On cherche une politique $\col{u} = \pi(\col{x}, t)$ qui minimise le coût $J$ pour tout l'espace d'état $\mathcal{X}_{ok}$ :
    \begin{equation}
        \pi(\col{x}, t) = \arg \min_{\col{u} \in \mathcal{U}_{ok}} J(\col{x}, t)
    \end{equation}
\end{definition}

En apprentissage par renforcement (\textit{RL}), cette politique est apprise par essai-erreur, tandis qu'en programmation dynamique, elle est calculée avec des méthodes numériques itératives en utilisant les équations. Ces méthodes sont très génériques et potentiellement très puissantes, mais en pratique le problème est souvent \textit{intractable} (le temps de calcul est trop long) et on doit chercher des solutions très approximées. Les techniques d'apprentissage par renforcement ont connu de grands succès récents, mais c'est typiquement un défi de les faire fonctionner et ce n'est pas la norme en date d'aujourd'hui (2026). Le sujet ne sera pas abordé en détail dans ces notes, il est le sujet de notes dédié au sujet disponible ici: \url{https://www.alexandregirard.com/teaching/dp/PDF/DP_Notes.pdf}

Alternativement, si on peut linéariser la dynamique du système, on peut utiliser une technique très pratique appelée la synthèse LQR, couramment utilisée en commande.

\begin{definition}{Régulateur Quadratique Linéaire (LQR)}{def:lqr_formal}
    Cas particulier où la dynamique est linéaire et le coût quadratique :
    \begin{align}
        f(\col{x}, \col{u}, t) &= \col{A}\col{x} + \col{B}\col{u} \\
        g(\col{x}, \col{u}) &= \col{x}^T \col{Q} \col{x} + \col{u}^T \col{R} \col{u} \\
        h(\col{x}(t_f)) &= \col{x}(t_f)^T \col{Q}_f \col{x}(t_f)
    \end{align}
    La solution se trouve analytiquement et est une politique linéaire optimale de la forme $\col{u} = -\col{K}\col{x}$.
\end{definition}

\subsection{Approches basées sur la trajectoire}

Plutôt que de chercher une solution universelle, ces méthodes se concentrent sur la recherche d'une trajectoire $\{\col{x}^*(t), \col{u}^*(t)\}$ partant d'une condition initiale spécifique $\col{x}_0$. 

\begin{definition}{Optimisation de trajectoire (Méthodes directes)}{def:direct_traj_formal}
    On discrétise le problème continu en $N$ segments de temps pour transformer le problème en programmation non-linéaire (NLP) :
    \begin{align*}
        \min_{\col{u}_0, \dots, \col{u}_{N-1}} \quad & \sum_{k=0}^{N-1} g_d(\col{x}_k, \col{u}_k) \Delta t + h(\col{x}_N) \\
        \text{s.c.} \quad & \col{x}_{k+1} = f_d(\col{x}_k, \col{u}_k) \\
        & \col{x}_k \in \mathcal{X}_{ok}, \quad \col{u}_k \in \mathcal{U}_{ok}
    \end{align*}
\end{definition}

Cette technique est abordée en détail au chapitre \ref{chap:trajopt}.


Il existe une autre famille de techniques moins courantes :

\begin{definition}{Méthodes indirectes}{def:indirect_traj_formal}
    On cherche à satisfaire les conditions nécessaires d'optimalité analytiques le long d'une trajectoire unique. Cela revient à résoudre un problème aux limites basé sur le principe du maximum de Pontryagin.
\end{definition}


\subsection{Hybrides}

En pratique, les approches basées sur une loi de commande et celles basées sur la trajectoire ne sont pas mutuellement exclusives. Elles sont souvent combinées pour bénéficier à la fois de la précision de la planification et de la robustesse des boucles fermées.

\begin{definition}{Optimisation de trajectoire + LQR}{def:traj_lqr}
    Cette approche hybride sépare la planification de la régulation en deux étapes distinctes :
    \begin{enumerate}
        \item \textbf{Planification (Hors-ligne/basse fréquence) :} On utilise l'optimisation de trajectoire pour trouver une trajectoire nominale optimale $\{\col{x}^*(t), \col{u}^*(t)\}$.
        \item \textbf{Régulation (Temps réel) :} On linéarise la dynamique autour de cette trajectoire pour concevoir un régulateur LQR variant dans le temps. La loi de commande devient :
        $$\col{u}(t) = \col{u}^*(t) - \col{K}(t)(\col{x}(t) - \col{x}^*(t))$$
    \end{enumerate}
    Le gain $\col{K}(t)$ permet de rejeter les perturbations et de maintenir le robot dans un « tube » de stabilité autour du plan initial.
\end{definition}

Alternativement, une autre stratégie pour donner un mécanisme de rétroaction est de constamment remesurer l'état actuel et ré-optimiser une trajectoire à partir de ce nouveau point :


\begin{definition}{Commande prédictive (MPC)}{def:mpc}
    La commande prédictive (\textit{Model Predictive Control} ou MPC) consiste à résoudre un problème d'optimisation de trajectoire en temps réel sur un horizon de temps glissant :
    \begin{itemize}
        \item À chaque instant $t$, on résout une optimisation sur un horizon court à partir de l'état actuel mesuré $\col{x}(t)$.
        \item On n'applique que la première commande $\col{u}_0$ de la séquence calculée.
        \item Au pas de temps suivant, on décale l'horizon et on recommence l'optimisation avec la nouvelle mesure de l'état.
    \end{itemize}
\end{definition}


Bien que le MPC repose mathématiquement sur l'optimisation de trajectoire, le fait de ré-optimiser à chaque pas de temps à partir de l'état réel transforme cette méthode en une \textbf{politique de rétroaction} implicite $\col{u} = \pi(\col{x})$. C'est l'un des outils les plus puissants pour gérer simultanément la dynamique et les contraintes $\mathcal{X}_{ok}$ et $\mathcal{U}_{ok}$.