\chapter{Commande optimale, programmation dynamique et apprentissage par renforcement}



\section{Introduction}

La programmation dynamique est un principe mathématique pour optimiser des décisions qui sont prises en séquence après avoir observé l'état d'un système. Le principe peut être utilisé autant pour analyser un système asservis classique, comme contrôler la position d'un moteur en choisissant la tension appliquée à ces bornes, que pour des problèmes probabiliste dans un contexte de finance, comme choisir quand acheter ou vendre une action, ou bien un problème d'IA comme choisir la pièce à déplacer lors d'une partie d'échec.

\subsection{Type de problèmes}

Le problème est de déterminer une loi de commande $c$, qui dicte l'action $\col{u}$ à prendre lorsque l'état du système est de $\col{x}$ au temps $t$:
\begin{align}
\col{u} = c( \col{x} , t )
\end{align}
de sorte à minimiser un coût intégral de la forme:
\begin{align}
    J=\int_0^{t_f}g(x,u,t) dt + h(x_f,t_f)
\end{align}

La plupart des outils sont mieux adapté à une approche de type temps discret. Cette section va donc présenter les principes et les algorithmes d'abord avec une approche à temps discret ou un index $k$ indique l'étape actuelle. Une approche pour travailler en temps continue est re-visitée plus tard à la section \ref{sec:dp_cont}. 

Le problème équivalent à résoudre en temps discret est de déterminer les lois de commande $c_k$, qui dictent l'action $\col{u}$ à prendre lorsque l'état du système est de $\col{x}$ à l'étape $k$:
\begin{align}
\col{u}_k = c_k( \col{x_k} )
\end{align}
de sorte à minimiser un coût additif de la forme:
\begin{align}
    J = \sum_{k=0}^{N-1} g_k(x_k, u_k , w_k) + g_N( x_N )
\end{align}

\subsection{Exemples}

\subsubsection{Système de chauffage optimal}

\subsubsection{Navigation optimale}


\subsection{Principe d'optimalité}

\begin{align}
\left[ \col{x}_0 , \col{x}_1 , ... , \col{x}_i , ... , \col{x}_N \right] \\
\left[ \col{u}_0 , \col{u}_1 , ... , \col{u}_i , ... , \col{x}_N \right] \\
\left[  \col{x}_i , ... , \col{x}_N \right] \\
\left[  \col{u}_i , ... , \col{u}_N \right]
\end{align}

\subsection{Programmation dynamique excate}

%%%%%%%%%%%%%%%%
\begin{align}
J^*_N(\col{x}_N) &= g_N(\col{x}_N) \\
J^*_k(\col{x}_k) &= 
\operatornamewithlimits{min}\limits_{\col{u}_k}
%\mathbb{E}
\left[
g_k(\col{x}_k , \col{u}_k ) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k ) 
}_{\col{x}_{k+1}}
)
\right] \\
u^*_k(\col{x}_k) &= 
\operatornamewithlimits{argmin}\limits_{\col{u}_k}
%\mathbb{E}
\left[
g_k(\col{x}_k , \col{u}_k ) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k ) 
}_{\col{x}_{k+1}}
)
\right] 
\label{eq:exactdp}
\end{align} 
%%%%%%%%%%%%%%%%%


\subsection{Variations sur un thème de programmation dynamique}

\paragraph{Stochastique}

%%%%%%%%%%%%%%%%
\begin{align}
J^*_k(\col{x}_k) = 
\operatornamewithlimits{min}\limits_{\col{u}_k}
{\color{red}
\operatornamewithlimits{E}\limits_{\col{w}_k}
}
%\mathbb{E}
&\left[
g_k(\col{x}_k , \col{u}_k , \col{w}_k ) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k , \col{w}_k ) 
}_{\col{x}_{k+1}}
)
\right] 
\end{align} 
%%%%%%%%%%%%%%%%%

\paragraph{Robuste}

%%%%%%%%%%%%%%%%
\begin{align}
J^*_k(\col{x}_k) = 
\operatornamewithlimits{min}\limits_{\col{u}_k}
{\color{red}
\operatornamewithlimits{max}\limits_{\col{w}_k}
}
%\mathbb{E}
&\left[
g_k(\col{x}_k , \col{u}_k , \col{w}_k ) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k , \col{w}_k ) 
}_{\col{x}_{k+1}}
)
\right] 
\end{align} 
%%%%%%%%%%%%%%%%%

\paragraph{À horizon de temps infini}

%%%%%%%%%%%%%%%%
\begin{align}
J^*(\col{x}) = 
\operatornamewithlimits{min}\limits_{\col{u}}
%\mathbb{E}
&\left[
g(\col{x} , \col{u} ) + {\color{red}\alpha} J^*( 
\underbrace{
f(\col{x} , \col{u} ) 
}_{\col{x}_{k+1}}
)
\right] 
\end{align} 
%%%%%%%%%%%%%%%%%

\paragraph{Sans modèles (apprentissage par renforcement)}

%%%%%%%%%%%%%%%%
\begin{align}
Q^*(\col{x}, \col{u} ) = g(\col{x} , \col{u} ) + 
\operatornamewithlimits{min}\limits_{\col{u}_{k+1}}
&\left[
Q^*( 
\underbrace{
f(\col{x} , \col{u} ) 
}_{\col{x}_{k+1}}
, \col{u}_{k+1}
)
\right] 
\end{align} 
%%%%%%%%%%%%%%%%%

\paragraph{À temps continu}

%%%%%%%%%%%%%%%%
\begin{align}
0 =
\operatornamewithlimits{min}\limits_{\col{u}}
%\mathbb{E}
\left[
g(\col{x} , \col{u} ) + \frac{\partial	J^*(\col{x},t)}{\partial \col{x} }
\underbrace{
f(\col{x} , \col{u} , t) )
}_{\dot{\col{x}}}
\right]
\label{eq:hjb}
\end{align} 
%%%%%%%%%%%%%%%%%

\subsection{Approches de programmation dynamique approximée}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problèmes déterministes}
Un grand ensemble de types de problèmes peuvent être converti..

%%%%%%%%%%%%%%%%
\begin{align}
J^*_k(i) &= 
\operatornamewithlimits{min}\limits_{j \in U(i)}
%\mathbb{E}
\left[
a_{ij}^k + J^*_{k+1}(j)
\right] \\
j^*_k(i) &= 
\operatornamewithlimits{argmin}\limits_{j \in U(i)}
%\mathbb{E}
\left[
a_{ij}^k + J^*_{k+1}(j)
\right] 
\label{eq:exactdpgraph}
\end{align} 
%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Programmation dynamique stochastique}


On cherche donc ici à minimiser l'espérance du coûts-à-venir:
\begin{align}
    J = \large{E} \left\{ \sum_{k=0}^{N-1} g_k(x_k, u_k , w_k) + g_N( x_N ) \right\}
\end{align}
avec l'algorithme de programmation dynamique suivant:
%%%%%%%%%%%%%%%%
\begin{align}
J^*_k(\col{x}_k) = 
\operatornamewithlimits{min}\limits_{\col{u}_k}
\operatornamewithlimits{E}\limits_{\col{w}_k}
%\mathbb{E}
\left[
g_k(\col{x}_k , \col{u}_k , \col{w}_k ) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k , \col{w}_k ) 
}_{\col{x}_{k+1}}
)
\right] \\
u^*_k(\col{x}_k) = 
\operatornamewithlimits{argmin}\limits_{\col{u}_k}
\operatornamewithlimits{E}\limits_{\col{w}_k}
%\mathbb{E}
\left[
g_k(\col{x}_k , \col{u}_k , \col{w}_k) + J^*_{k+1}( 
\underbrace{
f_k(\col{x}_k , \col{u}_k , \col{w}_k) 
}_{\col{x}_{k+1}}
)
\right] 
\end{align} 
%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Chaînes de Markov}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programmation dynamique robuste et approche minimax}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimisation sur un horizon de temps infini}

\begin{align}
    J = 
    \lim_{N\rightarrow \infty}
    \e{ \sum_{k=0}^{N} 
    \alpha^k
    g_k
    }
\end{align}

\subsection{Bellman equation}


\subsection{Algorithme d'itération de valeurs (\textit{Value-iteration})}

\subsection{Algorithme d'itération de loi de commande (\textit{policy-iteration})}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution LQR}

Dans le contexte d'un système dynamique linéaire à états/actions continus représenté par:
\begin{align}
    \col{x}_{k+1} = f_k(\col{x}_k , \col{u}_k , \col{w}_k )  = A_k \col{x}_k + B_k \col{u}_k + \col{w}_k
\end{align}
où $\col{x}_k$ et $\col{w}_k$ sont des vecteurs de dimension $n$ et $\col{w}_k$ un vecteur de dimension $m$. Le vecteur $\col{w}_k$ représente des perturbations aléatoires, indépendantes de l'état actuel, de l'action actuelle et de tous les états passés, et avec des distributions centrées autour de zéro: 
\begin{align}
    \e{\col{w}_k} = \col{0} %\quad \quad \e{w_i w_j} = 0 \; \forall \; i \neq j
\end{align}

Si on cherche donc à minimiser l'espérance du coût-à-venir:
\begin{align}
    J = \e{ \sum_{k=0}^{N-1} 
    \left(
    \underbrace{
    \col{x}_k^T Q_k \col{x}_k + \col{u}_k^T R_k \col{u}_k
    }_{ g_k( \col{x}_k, \col{u}_k , \col{w}_k ) }
    \right)
    +
    \underbrace{
    \col{x}_N^T Q_N \col{x}_N
    }_{ g_N( \col{x}_N )  }
    }
\end{align}
où les matrices $Q_k$ et $R_k$ sont symétriques et définies positives:
\begin{align}
    Q_k \geq 0 \quad \quad R_k > 0 
\end{align}

En appliquant l'algorithme de programmation dynamique (pour l'étape $N\rightarrow N-1$ ou une étape générique $k+1 \rightarrow k$), on trouve que:
1) le coût-à-venir d'un état $\col{x}_k$ a la forme quadratique suivante:
\begin{align}
    J_k^*( \col{x}_k ) = \col{x}_k^T S_k \col{x}_k + c
\end{align}
où $S_k$ est une matrice symétrique qui caractérise le coût-à-venir à l'état $\col{x}_k$ et $c$ est une constante qui ne dépent pas de l'état actuel.
2) la loi de commande optimale a la forme linéaire suivante:
\begin{align}
    \col{u}_k^* = c_k^*( \col{x}_k ) = - K_k \col{x}_k
\end{align}
où $K_k$ est la matrice de gain égale à
\begin{align}
    K_k = \left[ R_k + B_k^T S_{k+1} B_k \right]^{-1} B_k S_{k+1} A_k
\end{align}

3) la matrice $S_k$ dans les équations précédentes peut être calculée en partant du coût final à $k=N$ et en remontant dans le temps avec la récursion suivante:
\begin{align}
    S_k = Q_k + A_k^T \left( S_{k+1} - S_{k+1}^T B_k^T  \left[ R_k + B_k^T S_{k+1} B_k \right]^{-1} B_k S_{k+1} \right) A_k
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Apprentissage par renforcement}


\subsection{TD-Learning}

\subsection{Q-Learning}

\subsection{Sarsa}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programmation dynamique pour un système à temps continu}
\label{sec:dp_continu}

\subsection{Hamilton–Jacobi–Bellman equation}

%%%%%%%%%%%%%%%%
\begin{align}
0 =
\operatornamewithlimits{min}\limits_{\col{u}}
%\mathbb{E}
\left[
g(\col{x} , \col{u} ) + \frac{\partial	J^*(\col{x},t)}{\partial \col{x} }
\underbrace{
f(\col{x} , \col{u} , t) )
}_{\dot{\col{x}}}
\right]
\label{eq:hjb}
\end{align} 
%%%%%%%%%%%%%%%%%


\subsection{Exercises}